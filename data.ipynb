{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a359ec3",
   "metadata": {},
   "source": [
    "Data Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74669af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Collecte du domaine : machine-learning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_21100\\835704994.py:119: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  repo[\"collected_at\"] = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 150...\n",
      "Collectés : 200...\n",
      "Collectés : 250...\n",
      "Collectés : 300...\n",
      "Collectés : 350...\n",
      "Collectés : 400...\n",
      "Collectés : 450...\n",
      "Collectés : 455...\n",
      "\n",
      "--- Collecte du domaine : data-engineering ---\n",
      "Collectés : 50...\n",
      "Collectés : 88...\n",
      "\n",
      "--- Collecte du domaine : deep-learning ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 150...\n",
      "Collectés : 200...\n",
      "Collectés : 250...\n",
      "Collectés : 300...\n",
      "Collectés : 301...\n",
      "\n",
      "--- Collecte du domaine : devops ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 150...\n",
      "Collectés : 200...\n",
      "Collectés : 250...\n",
      "Collectés : 300...\n",
      "Collectés : 325...\n",
      "\n",
      "--- Collecte du domaine : cloud ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "⚠️ Erreur GraphQL (1/3) : ('Connection broken: IncompleteRead(1501 bytes read, 8739 more expected)', IncompleteRead(1501 bytes read, 8739 more expected))\n",
      "Collectés : 150...\n",
      "Collectés : 200...\n",
      "Collectés : 250...\n",
      "Collectés : 300...\n",
      "Collectés : 350...\n",
      "Collectés : 400...\n",
      "Collectés : 450...\n",
      "Collectés : 500...\n",
      "Collectés : 550...\n",
      "Collectés : 600...\n",
      "Collectés : 650...\n",
      "Collectés : 700...\n",
      "Collectés : 750...\n",
      "\n",
      "--- Collecte du domaine : blockchain ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 150...\n",
      "Collectés : 200...\n",
      "Collectés : 250...\n",
      "Collectés : 254...\n",
      "\n",
      "--- Collecte du domaine : cybersecurity ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 110...\n",
      "\n",
      "--- Collecte du domaine : natural-language-processing ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 150...\n",
      "Collectés : 178...\n",
      "\n",
      "--- Collecte du domaine : computer-vision ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 150...\n",
      "Collectés : 200...\n",
      "Collectés : 221...\n",
      "\n",
      "--- Collecte du domaine : internet-of-things ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 112...\n",
      "\n",
      "--- Collecte du domaine : automation ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 150...\n",
      "Collectés : 200...\n",
      "Collectés : 250...\n",
      "Collectés : 300...\n",
      "Collectés : 350...\n",
      "Collectés : 400...\n",
      "Collectés : 450...\n",
      "Collectés : 500...\n",
      "Collectés : 544...\n",
      "\n",
      "--- Collecte du domaine : business-intelligence ---\n",
      "Collectés : 50...\n",
      "Collectés : 93...\n",
      "\n",
      "--- Collecte du domaine : big-data ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 150...\n",
      "Collectés : 178...\n",
      "\n",
      "--- Collecte du domaine : recommendation-systems ---\n",
      "Collectés : 38...\n",
      "\n",
      "--- Collecte du domaine : fraud-detection ---\n",
      "Collectés : 19...\n",
      "\n",
      "--- Collecte du domaine : game-development ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 150...\n",
      "Collectés : 185...\n",
      "\n",
      "--- Collecte du domaine : scientific-computing ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 107...\n",
      "\n",
      "--- Collecte du domaine : web-development ---\n",
      "Collectés : 50...\n",
      "Collectés : 100...\n",
      "Collectés : 150...\n",
      "Collectés : 200...\n",
      "Collectés : 250...\n",
      "Collectés : 300...\n",
      "Collectés : 350...\n",
      "Collectés : 400...\n",
      "Collectés : 416...\n",
      "\n",
      "--- Collecte du domaine : mobile-development ---\n",
      "Collectés : 50...\n",
      "⚠️ Erreur GraphQL (1/3) : HTTPSConnectionPool(host='api.github.com', port=443): Read timed out. (read timeout=15)\n",
      "Collectés : 88...\n",
      "\n",
      "--- Collecte du domaine : iot-smart-devices ---\n",
      "Collectés : 0...\n",
      "\n",
      "✅ Collecte terminée : 4462 repositories\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "\n",
    "TOKEN = \"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\"\n",
    "URL_GRAPHQL = \"https://api.github.com/graphql\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "OUTPUT_FILE = \"github_data_raw.jsonl\"\n",
    "\n",
    "# =========================\n",
    "# GRAPHQL QUERY\n",
    "# =========================\n",
    "\n",
    "QUERY = \"\"\"\n",
    "query ($queryString: String!, $cursor: String) {\n",
    "  search(query: $queryString, type: REPOSITORY, first: 50, after: $cursor) {\n",
    "    pageInfo {\n",
    "      hasNextPage\n",
    "      endCursor\n",
    "    }\n",
    "    edges {\n",
    "      node {\n",
    "        ... on Repository {\n",
    "          databaseId\n",
    "          nameWithOwner\n",
    "          url\n",
    "          description\n",
    "          stargazerCount\n",
    "          forkCount\n",
    "          pushedAt\n",
    "          languages(first: 5, orderBy: {field: SIZE, direction: DESC}) {\n",
    "            nodes { name }\n",
    "          }\n",
    "          repositoryTopics(first: 10) {\n",
    "            nodes { topic { name } }\n",
    "          }\n",
    "          object(expression: \"HEAD:README.md\") {\n",
    "            ... on Blob { text }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# SAFE GRAPHQL REQUEST\n",
    "# =========================\n",
    "\n",
    "def safe_graphql_request(query, variables, retries=3, sleep=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                URL_GRAPHQL,\n",
    "                json={\"query\": query, \"variables\": variables},\n",
    "                headers=HEADERS,\n",
    "                timeout=15\n",
    "            )\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(response.text)\n",
    "\n",
    "            data = response.json()\n",
    "\n",
    "            if \"errors\" in data:\n",
    "                raise Exception(data[\"errors\"])\n",
    "\n",
    "            return data[\"data\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur GraphQL ({attempt+1}/{retries}) : {e}\")\n",
    "            time.sleep(sleep)\n",
    "\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# SAVE REPO (JSONL)\n",
    "# =========================\n",
    "\n",
    "def save_repo(repo):\n",
    "    with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(repo, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# =========================\n",
    "# FETCH REPOSITORIES\n",
    "# =========================\n",
    "\n",
    "def fetch_repos(query_str, domain, limit=500):\n",
    "    repos_count = 0\n",
    "    cursor = None\n",
    "\n",
    "    while repos_count < limit:\n",
    "        variables = {\n",
    "            \"queryString\": query_str,\n",
    "            \"cursor\": cursor\n",
    "        }\n",
    "\n",
    "        data = safe_graphql_request(QUERY, variables)\n",
    "\n",
    "        if data is None:\n",
    "            print(\"❌ Abandon de ce domaine (trop d’erreurs)\")\n",
    "            break\n",
    "\n",
    "        search = data[\"search\"]\n",
    "\n",
    "        for edge in search[\"edges\"]:\n",
    "            repo = edge[\"node\"]\n",
    "            repo[\"collected_at\"] = datetime.utcnow().isoformat()\n",
    "            repo[\"target_domain\"] = domain  # ✅ Assignation correcte ici\n",
    "            save_repo(repo)\n",
    "            repos_count += 1\n",
    "\n",
    "            if repos_count >= limit:\n",
    "                break\n",
    "\n",
    "        print(f\"Collectés : {repos_count}...\")\n",
    "\n",
    "        if not search[\"pageInfo\"][\"hasNextPage\"]:\n",
    "            break\n",
    "\n",
    "        cursor = search[\"pageInfo\"][\"endCursor\"]\n",
    "        time.sleep(1.2)  # rate limit safe\n",
    "\n",
    "    return repos_count\n",
    "\n",
    "# =========================\n",
    "# DOMAIN COLLECTION\n",
    "# =========================\n",
    "\n",
    "domains = [\n",
    "    \"machine-learning\",\n",
    "    \"data-engineering\",\n",
    "    \"deep-learning\",\n",
    "    \"devops\",\n",
    "    \"cloud\",\n",
    "    \"blockchain\",\n",
    "    \"cybersecurity\",\n",
    "    \"natural-language-processing\",\n",
    "    \"computer-vision\",\n",
    "    \"internet-of-things\",\n",
    "    \"automation\",\n",
    "    \"business-intelligence\",\n",
    "    \"big-data\",\n",
    "    \"recommendation-systems\",\n",
    "    \"fraud-detection\",\n",
    "    \"game-development\",\n",
    "    \"scientific-computing\",\n",
    "    \"web-development\",\n",
    "    \"mobile-development\",\n",
    "    \"iot-smart-devices\"\n",
    "]\n",
    "\n",
    "def collect_by_domains(domain_list, limit_per_domain=500):\n",
    "    total = 0\n",
    "\n",
    "    for domain in domain_list:\n",
    "        print(f\"\\n--- Collecte du domaine : {domain} ---\")\n",
    "\n",
    "        query_string = (\n",
    "            f\"{domain} in:name,description,readme \"\n",
    "            f\"fork:false stars:>20 \"\n",
    "            f\"-library -framework -boilerplate\"\n",
    "        )\n",
    "\n",
    "        collected = fetch_repos(query_string, domain, limit=limit_per_domain)\n",
    "        total += collected\n",
    "\n",
    "    print(f\"\\n✅ Collecte terminée : {total} repositories\")\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collect_by_domains(domains, limit_per_domain=750)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02fd5b",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ad512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw repos count: 4462\n",
      "After deduplication: 1350\n",
      "After removing repos without text: 1349\n",
      "✅ Preprocessing finished, saved to: github_data_cleaned.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "INPUT_FILE = \"github_data_raw.jsonl\"  # your raw JSONL file\n",
    "OUTPUT_FILE = \"github_data_cleaned.jsonl\"  # cleaned dataset\n",
    "\n",
    "# =========================\n",
    "# FUNCTION TO CLEAN TEXT\n",
    "# =========================\n",
    "def clean_text(text, max_len=3000):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()  # lowercase\n",
    "    # Remove Markdown, badges, URLs, multiple spaces\n",
    "    text = re.sub(r\"#|\\*|`|>|-|\\!\\[.*?\\]\\(.*?\\)\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text[:max_len]\n",
    "\n",
    "# =========================\n",
    "# LOAD RAW DATA\n",
    "# =========================\n",
    "repos = []\n",
    "with open(INPUT_FILE, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        repos.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(repos)\n",
    "print(f\"Raw repos count: {len(df)}\")\n",
    "\n",
    "# =========================\n",
    "# DROP DUPLICATES\n",
    "# =========================\n",
    "df = df.drop_duplicates(subset=['databaseId'])\n",
    "print(f\"After deduplication: {len(df)}\")\n",
    "\n",
    "# =========================\n",
    "# CLEAN DESCRIPTION AND README\n",
    "# =========================\n",
    "def get_readme_text(obj):\n",
    "    if obj and 'text' in obj:\n",
    "        return obj['text']\n",
    "    return \"\"\n",
    "\n",
    "df['description_clean'] = df['description'].apply(clean_text)\n",
    "df['readme_clean'] = df['object'].apply(lambda x: clean_text(get_readme_text(x)))\n",
    "\n",
    "# =========================\n",
    "# EXTRACT LANGUAGES AND TOPICS\n",
    "# =========================\n",
    "df['languages_list'] = df['languages'].apply(\n",
    "    lambda x: [l['name'] for l in x['nodes']] if x and 'nodes' in x and x['nodes'] else []\n",
    ")\n",
    "df['topics_list'] = df['repositoryTopics'].apply(\n",
    "    lambda x: [t['topic']['name'] for t in x['nodes']] if x and 'nodes' in x and x['nodes'] else []\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# CREATE FULL_TEXT FOR EMBEDDINGS\n",
    "# =========================\n",
    "df['full_text'] = (\n",
    "    df['description_clean'] + \" \" +\n",
    "    df['readme_clean'] + \" \" +\n",
    "    df['topics_list'].apply(lambda x: \" \".join(x)) + \" \" +\n",
    "    df['languages_list'].apply(lambda x: \" \".join(x))\n",
    ")\n",
    "\n",
    "# Drop repos with empty full_text\n",
    "df = df[df['full_text'].str.strip() != \"\"]\n",
    "print(f\"After removing repos without text: {len(df)}\")\n",
    "\n",
    "# =========================\n",
    "# NUMERIC FEATURES\n",
    "# =========================\n",
    "df['log_stars'] = np.log1p(df['stargazerCount'])\n",
    "df['fork_ratio'] = df['forkCount'] / (df['stargazerCount'] + 1)\n",
    "\n",
    "df['pushedAt'] = pd.to_datetime(df['pushedAt'])\n",
    "df['recency_days'] = (datetime.now(timezone.utc) - df['pushedAt']).dt.days\n",
    "\n",
    "# =========================\n",
    "# KEEP ONLY USEFUL COLUMNS\n",
    "# =========================\n",
    "final_cols = [\n",
    "    'databaseId', 'nameWithOwner', 'url', 'target_domain',\n",
    "    'full_text', 'languages_list', 'topics_list',\n",
    "    'stargazerCount', 'forkCount', 'log_stars', 'fork_ratio', 'recency_days'\n",
    "]\n",
    "df_final = df[final_cols]\n",
    "\n",
    "# =========================\n",
    "# SAVE CLEANED DATA\n",
    "# =========================\n",
    "df_final.to_json(OUTPUT_FILE, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"✅ Preprocessing finished, saved to: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0260d59",
   "metadata": {},
   "source": [
    "Enrechissement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "003d0420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilisé : cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6350078e91aa4e25a258a2646472a607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9731091d294e432d94f30436dd02cc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enrichissement terminé sur GPU si disponible\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import swifter  # pip install swifter pour parallélisation\n",
    "\n",
    "# --- 1️⃣ Détecter le GPU et charger le modèle dessus ---\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device utilisé :\", device)\n",
    "\n",
    "# Charger le modèle MiniLM sur GPU si dispo\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "kw_model = KeyBERT(model)\n",
    "\n",
    "# --- 2️⃣ Fonction d'enrichissement ---\n",
    "def enrich_repo_metadata(df):\n",
    "    \"\"\"\n",
    "    Enrichit les colonnes tools_list et topics_list à partir de full_text\n",
    "    et des informations existantes (languages_list et repositoryTopics)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Construire sets uniques des langages et topics déjà présents ---\n",
    "    all_languages = set()\n",
    "    for langs in df['languages_list']:\n",
    "        all_languages.update(langs)\n",
    "\n",
    "    all_topics = set()\n",
    "    for topics in df['topics_list']:\n",
    "        all_topics.update(topics)\n",
    "\n",
    "    # --- Fonction pour extraire tools automatiquement ---\n",
    "    def extract_tools(text):\n",
    "        tools = set()\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # Ajouter langages connus\n",
    "        for lang in all_languages:\n",
    "            if lang.lower() in text_lower:\n",
    "                tools.add(lang)\n",
    "\n",
    "        # Ajouter topics existants\n",
    "        for topic in all_topics:\n",
    "            if topic.lower() in text_lower:\n",
    "                tools.add(topic)\n",
    "\n",
    "        # Extraire keywords du texte (frameworks, libs, etc.)\n",
    "        kws = kw_model.extract_keywords(text, keyphrase_ngram_range=(1,2), top_n=10)\n",
    "        for kw in kws:\n",
    "            tools.add(kw[0])\n",
    "        return list(tools)\n",
    "\n",
    "    # --- Fonction pour extraire topics automatiquement ---\n",
    "    def extract_topics(text, top_n=10):\n",
    "        if not text.strip():\n",
    "            return []\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            text,\n",
    "            keyphrase_ngram_range=(1,2),\n",
    "            stop_words='english',\n",
    "            top_n=top_n\n",
    "        )\n",
    "        return [kw[0] for kw in keywords]\n",
    "\n",
    "    # --- 3️⃣ Appliquer sur le dataframe avec parallélisation swifter ---\n",
    "    df['tools_list'] = df['full_text'].swifter.apply(extract_tools)\n",
    "    df['topics_list'] = df['full_text'].swifter.apply(lambda x: extract_topics(x, top_n=10))\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- 4️⃣ Charger le fichier JSON, enrichir et sauvegarder ---\n",
    "df = pd.read_json(\"github_data_cleaned.jsonl\", lines=True)\n",
    "df = enrich_repo_metadata(df)\n",
    "df.to_json(\"github_data_enriched.jsonl\", orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(\"✅ Enrichissement terminé sur GPU si disponible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1c21b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Nettoyage terminé et languages_list supprimée\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier enrichi\n",
    "df = pd.read_json(\"github_data_enriched.jsonl\", lines=True)\n",
    "\n",
    "# --- Stoplist : mots génériques à ignorer ---\n",
    "stop_words = {\n",
    "    'list', 'software', 'course', 'blog', 'request', 'new', 'it', 'professional',\n",
    "    'free', 'books', 'learning', 'data', 'resource', 'awesome', 'curated', 'news'\n",
    "}\n",
    "\n",
    "# --- Fonction de nettoyage ---\n",
    "def clean_tools(tools):\n",
    "    if not tools:\n",
    "        return []\n",
    "    # 1️⃣ minuscules\n",
    "    tools = [t.lower() for t in tools]\n",
    "    # 2️⃣ supprimer doublons\n",
    "    tools = list(set(tools))\n",
    "    # 3️⃣ supprimer stopwords\n",
    "    tools = [t for t in tools if t not in stop_words]\n",
    "    # 4️⃣ tri alphabétique\n",
    "    tools.sort()\n",
    "    return tools\n",
    "\n",
    "# --- Appliquer le nettoyage ---\n",
    "df['tools_list'] = df['tools_list'].apply(clean_tools)\n",
    "\n",
    "# --- Supprimer la colonne languages_list ---\n",
    "if 'languages_list' in df.columns:\n",
    "    df.drop(columns=['languages_list'], inplace=True)\n",
    "\n",
    "# --- Sauvegarder le résultat ---\n",
    "df.to_json(\"github_data_cleaned_tools.jsonl\", orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(\"✅ Nettoyage terminé et languages_list supprimée\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53037694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "databaseId                                                 21872392\n",
      "nameWithOwner                 josephmisiti/awesome-machine-learning\n",
      "url               https://github.com/josephmisiti/awesome-machin...\n",
      "target_domain                                      machine-learning\n",
      "full_text         a curated list of awesome machine learning fra...\n",
      "topics_list       [learning curated, learning frameworks, learni...\n",
      "stargazerCount                                                71548\n",
      "forkCount                                                     15288\n",
      "log_stars                                                 11.178138\n",
      "fork_ratio                                                 0.213672\n",
      "recency_days                                                      2\n",
      "tools_list        [ai, ast, awesome machine, blogs, book, c, c++...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier enrichi\n",
    "df = pd.read_json(\"github_data_cleaned_tools.jsonl\", lines=True)\n",
    "# afficher le premier enregistrement\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e871f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
